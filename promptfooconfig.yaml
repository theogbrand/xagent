# This configuration compares LLM output of 2 prompts x 2 GPT models across 3 test cases.
# Learn more: https://promptfoo.dev/docs/configuration/guide
description: 'My first eval'

prompts:
  - file://prompt.txt
  # - Define {{word}} in 1-4 words
  # - Explain {{word}} in 1-4 words
  # - "Write a very concise, funny tweet about {{topic}}"

providers:
  # - openai:gpt-3.5-turbo-0613
  # - openai:gpt-4
  - id: anthropic:messages:claude-3-opus-20240229
    config:
      temperature: 0.0
      max_tokens: 512

tests:
  - vars:
      word: abound

  # - vars:
  #     topic: avocado toast
  #   assert:
  #     # For more information on assertions, see https://promptfoo.dev/docs/configuration/expected-outputs
  #     - type: icontains
  #       value: avocado
  #     - type: javascript
  #       value: 1 / (output.length + 1)  # prefer shorter outputs

  # - vars:
  #     topic: new york city
  #   assert:
  #     # For more information on model-graded evals, see https://promptfoo.dev/docs/configuration/expected-outputs/model-graded
  #     - type: llm-rubric
  #       value: ensure that the output is funny
